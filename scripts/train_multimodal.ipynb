{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12312ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, classification_report, roc_auc_score)\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"UNIFIED MULTIMODAL PIPELINE - ALL 5 FOLDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =====================================================\n",
    "# SET RANDOM SEEDS\n",
    "# =====================================================\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "# GPU Configuration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "        tf.keras.mixed_precision.set_global_policy('float32')\n",
    "        print(\"GPU configured (float32, deterministic mode)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU error: {e}\")\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION \n",
    "# =====================================================\n",
    "\n",
    "EEG_CSV = '/home/jupyter-yin10/EEG_HAR/Overall_Multimodal_pipeline/data/EEG_combined_all.csv'\n",
    "ACG_CSV = '/home/jupyter-yin10/EEG_HAR/Overall_Multimodal_pipeline/data/ACG_combined.csv'\n",
    "GYRO_CSV = '/home/jupyter-yin10/EEG_HAR/Overall_Multimodal_pipeline/data/GYRO_combined.csv'\n",
    "IMPORTANCE_FILE = \"/home/jupyter-yin10/EEG_HAR/Overall_Multimodal_pipeline/data/feature_importance_summary.csv\"\n",
    "OUTPUT_DIR = '/home/jupyter-yin10/EEG_HAR/Overall_Multimodal_pipeline/unified_multimodal_5folds'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Window parameters\n",
    "WINDOW_SIZE_SEC = 4\n",
    "OVERLAP = 0.0  # 0% overlap\n",
    "\n",
    "# EEG parameters\n",
    "EEG_FS = 125\n",
    "EEG_WINDOW_SAMPLES = int(WINDOW_SIZE_SEC * EEG_FS)  # 500\n",
    "EEG_CHANNELS = 16\n",
    "\n",
    "# HAR parameters\n",
    "HAR_FS = 25\n",
    "HAR_WINDOW_SAMPLES = int(WINDOW_SIZE_SEC * HAR_FS)  # 100\n",
    "\n",
    "# Model parameters\n",
    "D_MODEL = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 2\n",
    "FF_DIM = 256\n",
    "EEGNET_DROPOUT = 0.5\n",
    "TRANSFORMER_DROPOUT = 0.3\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "PATIENCE = 15\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Window: {WINDOW_SIZE_SEC}s, Overlap: {OVERLAP*100:.0f}%\")\n",
    "print(f\"  EEG: {EEG_WINDOW_SAMPLES} samples at {EEG_FS}Hz, {EEG_CHANNELS} channels\")\n",
    "print(f\"  HAR: {HAR_WINDOW_SAMPLES} samples at {HAR_FS}Hz\")\n",
    "print(f\"  Training: All {N_FOLDS} folds\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: LOAD RAW DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "eeg_df = pd.read_csv(EEG_CSV)\n",
    "acg_df = pd.read_csv(ACG_CSV)\n",
    "gyro_df = pd.read_csv(GYRO_CSV)\n",
    "\n",
    "print(f\"Loaded:\")\n",
    "print(f\"  EEG: {eeg_df.shape}\")\n",
    "print(f\"  ACG: {acg_df.shape}\")\n",
    "print(f\"  GYRO: {gyro_df.shape}\")\n",
    "\n",
    "# Get activity mapping\n",
    "activity_mapping = eeg_df[['activity_id', 'activity_label']].drop_duplicates().set_index('activity_id')['activity_label'].to_dict()\n",
    "N_CLASSES = len(activity_mapping)\n",
    "ACTIVITY_LABELS = [activity_mapping[i] for i in sorted(activity_mapping.keys())]\n",
    "\n",
    "print(f\"\\nActivities ({N_CLASSES} classes): {ACTIVITY_LABELS}\")\n",
    "\n",
    "# Save activity mapping\n",
    "with open(os.path.join(OUTPUT_DIR, 'activity_mapping.json'), 'w') as f:\n",
    "    json.dump(activity_mapping, f, indent=2)\n",
    "\n",
    "# =====================================================\n",
    "# CREATE FEATURE NAME MAPPING\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING FEATURE NAME MAPPING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_feature_names():\n",
    "    \"\"\"Create feature names matching the importance file convention\"\"\"\n",
    "    feature_names = []\n",
    "    \n",
    "    # ACG 3D features (16 features)\n",
    "    acg_3d_names = ['mag_mean', 'mag_std', 'mag_max', 'mag_min', \n",
    "                    'jerk_mean', 'jerk_std', 'jerk_max', \n",
    "                    'pitch_mean', 'pitch_std', 'roll_mean', 'roll_std', \n",
    "                    'sma', 'corr_12', 'corr_13', 'corr_23', 'energy']\n",
    "    feature_names.extend([f'acg_3d_{name}' for name in acg_3d_names])\n",
    "    \n",
    "    # ACG time-domain (11 features × 3 axes = 33 features)\n",
    "    time_feats = ['mean', 'std', 'min', 'max', 'median', 'range', 'rms', \n",
    "                  'skewness', 'kurtosis', 'zcr', 'mad']\n",
    "    for axis_num in [1, 2, 3]:\n",
    "        feature_names.extend([f'acg_axis{axis_num}_time_{feat}' for feat in time_feats])\n",
    "    \n",
    "    # ACG frequency-domain (8 features × 3 axes = 24 features)\n",
    "    freq_feats = ['dominant_freq', 'spectral_energy', 'spectral_entropy', \n",
    "                  'mean_freq', 'median_freq', 'power_0_5hz', 'power_5_10hz', \n",
    "                  'power_10_12_5hz']\n",
    "    for axis_num in [1, 2, 3]:\n",
    "        feature_names.extend([f'acg_axis{axis_num}_freq_{feat}' for feat in freq_feats])\n",
    "    \n",
    "    # GYRO 3D features (16 features)\n",
    "    feature_names.extend([f'gyro_3d_{name}' for name in acg_3d_names])\n",
    "    \n",
    "    # GYRO time-domain (11 features × 3 axes = 33 features)\n",
    "    for axis_num in [1, 2, 3]:\n",
    "        feature_names.extend([f'gyro_axis{axis_num}_time_{feat}' for feat in time_feats])\n",
    "    \n",
    "    # GYRO frequency-domain (8 features × 3 axes = 24 features)\n",
    "    for axis_num in [1, 2, 3]:\n",
    "        feature_names.extend([f'gyro_axis{axis_num}_freq_{feat}' for feat in freq_feats])\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "all_feature_names = create_feature_names()\n",
    "print(f\"Total features in extraction order: {len(all_feature_names)}\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 2: CREATE ALIGNED WINDOWS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: CREATE ALIGNED WINDOWS (0% OVERLAP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get unique recordings\n",
    "eeg_recordings = set(eeg_df.groupby(['subject', 'activity_id']).size().index)\n",
    "acg_recordings = set(acg_df.groupby(['subject', 'activity_id']).size().index)\n",
    "gyro_recordings = set(gyro_df.groupby(['subject', 'activity_id']).size().index)\n",
    "\n",
    "common_recordings = sorted(list(eeg_recordings & acg_recordings & gyro_recordings))\n",
    "\n",
    "print(f\"\\nCommon recordings: {len(common_recordings)}\")\n",
    "\n",
    "# Storage for aligned windows\n",
    "all_eeg_windows = []\n",
    "all_har_features = []\n",
    "all_labels = []\n",
    "all_recording_ids = []\n",
    "\n",
    "eeg_columns = [f'ch{i}' for i in range(1, 17)]\n",
    "\n",
    "# HAR feature extraction functions\n",
    "def extract_time_domain_features(signal):\n",
    "    features = {}\n",
    "    features['mean'] = np.mean(signal)\n",
    "    features['std'] = np.std(signal)\n",
    "    features['min'] = np.min(signal)\n",
    "    features['max'] = np.max(signal)\n",
    "    features['median'] = np.median(signal)\n",
    "    features['range'] = np.max(signal) - np.min(signal)\n",
    "    features['rms'] = np.sqrt(np.mean(signal**2))\n",
    "    features['skewness'] = stats.skew(signal)\n",
    "    features['kurtosis'] = stats.kurtosis(signal)\n",
    "    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n",
    "    features['zcr'] = len(zero_crossings) / len(signal)\n",
    "    features['mad'] = np.mean(np.abs(signal - np.mean(signal)))\n",
    "    return features\n",
    "\n",
    "def extract_frequency_domain_features(signal, sampling_rate):\n",
    "    features = {}\n",
    "    N = len(signal)\n",
    "    yf = fft(signal)\n",
    "    xf = fftfreq(N, 1/sampling_rate)\n",
    "    \n",
    "    pos_mask = xf > 0\n",
    "    freqs = xf[pos_mask]\n",
    "    magnitude = np.abs(yf[pos_mask])\n",
    "    power = magnitude**2\n",
    "    \n",
    "    dominant_idx = np.argmax(magnitude)\n",
    "    features['dominant_freq'] = freqs[dominant_idx]\n",
    "    features['spectral_energy'] = np.sum(power)\n",
    "    \n",
    "    psd_norm = power / np.sum(power)\n",
    "    psd_norm = psd_norm[psd_norm > 0]\n",
    "    features['spectral_entropy'] = -np.sum(psd_norm * np.log2(psd_norm))\n",
    "    \n",
    "    features['mean_freq'] = np.sum(freqs * magnitude) / np.sum(magnitude)\n",
    "    \n",
    "    cumsum_power = np.cumsum(power)\n",
    "    median_idx = np.where(cumsum_power >= cumsum_power[-1] / 2)[0][0]\n",
    "    features['median_freq'] = freqs[median_idx]\n",
    "    \n",
    "    band1_mask = (freqs >= 0) & (freqs < 5)\n",
    "    features['power_0_5hz'] = np.sum(power[band1_mask])\n",
    "    \n",
    "    band2_mask = (freqs >= 5) & (freqs < 10)\n",
    "    features['power_5_10hz'] = np.sum(power[band2_mask])\n",
    "    \n",
    "    band3_mask = (freqs >= 10) & (freqs < 12.5)\n",
    "    features['power_10_12_5hz'] = np.sum(power[band3_mask])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_3d_features(window):\n",
    "    axis1, axis2, axis3 = window[:, 0], window[:, 1], window[:, 2]\n",
    "    features = {}\n",
    "    \n",
    "    magnitude = np.sqrt(axis1**2 + axis2**2 + axis3**2)\n",
    "    features['mag_mean'] = np.mean(magnitude)\n",
    "    features['mag_std'] = np.std(magnitude)\n",
    "    features['mag_max'] = np.max(magnitude)\n",
    "    features['mag_min'] = np.min(magnitude)\n",
    "    \n",
    "    jerk = np.diff(magnitude)\n",
    "    features['jerk_mean'] = np.mean(np.abs(jerk))\n",
    "    features['jerk_std'] = np.std(jerk)\n",
    "    features['jerk_max'] = np.max(np.abs(jerk))\n",
    "    \n",
    "    pitch = np.arctan2(axis2, np.sqrt(axis1**2 + axis3**2))\n",
    "    roll = np.arctan2(axis1, np.sqrt(axis2**2 + axis3**2))\n",
    "    \n",
    "    features['pitch_mean'] = np.mean(pitch)\n",
    "    features['pitch_std'] = np.std(pitch)\n",
    "    features['roll_mean'] = np.mean(roll)\n",
    "    features['roll_std'] = np.std(roll)\n",
    "    \n",
    "    features['sma'] = np.sum(np.abs(axis1) + np.abs(axis2) + np.abs(axis3)) / len(axis1)\n",
    "    \n",
    "    features['corr_12'] = np.corrcoef(axis1, axis2)[0, 1]\n",
    "    features['corr_13'] = np.corrcoef(axis1, axis3)[0, 1]\n",
    "    features['corr_23'] = np.corrcoef(axis2, axis3)[0, 1]\n",
    "    \n",
    "    features['energy'] = np.sum(axis1**2 + axis2**2 + axis3**2) / len(axis1)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_all_har_features(acg_window, gyro_window, sampling_rate):\n",
    "    \"\"\"Extract all 152 features for HAR in the correct order\"\"\"\n",
    "    all_features = []\n",
    "    \n",
    "    # ACG 3D features (16)\n",
    "    acg_3d = extract_3d_features(acg_window)\n",
    "    for val in acg_3d.values():\n",
    "        all_features.append(val)\n",
    "    \n",
    "    # ACG time-domain per axis (11 × 3 = 33)\n",
    "    for axis in range(3):\n",
    "        time_feats = extract_time_domain_features(acg_window[:, axis])\n",
    "        for val in time_feats.values():\n",
    "            all_features.append(val)\n",
    "    \n",
    "    # ACG frequency-domain per axis (8 × 3 = 24)\n",
    "    for axis in range(3):\n",
    "        freq_feats = extract_frequency_domain_features(acg_window[:, axis], sampling_rate)\n",
    "        for val in freq_feats.values():\n",
    "            all_features.append(val)\n",
    "    \n",
    "    # GYRO 3D features (16)\n",
    "    gyro_3d = extract_3d_features(gyro_window)\n",
    "    for val in gyro_3d.values():\n",
    "        all_features.append(val)\n",
    "    \n",
    "    # GYRO time-domain per axis (11 × 3 = 33)\n",
    "    for axis in range(3):\n",
    "        time_feats = extract_time_domain_features(gyro_window[:, axis])\n",
    "        for val in time_feats.values():\n",
    "            all_features.append(val)\n",
    "    \n",
    "    # GYRO frequency-domain per axis (8 × 3 = 24)\n",
    "    for axis in range(3):\n",
    "        freq_feats = extract_frequency_domain_features(gyro_window[:, axis], sampling_rate)\n",
    "        for val in freq_feats.values():\n",
    "            all_features.append(val)\n",
    "    \n",
    "    return np.array(all_features)\n",
    "\n",
    "print(\"\\nProcessing recordings and creating aligned windows...\")\n",
    "\n",
    "skipped = 0\n",
    "processed = 0\n",
    "\n",
    "for rec_idx, (subject, activity_id) in enumerate(tqdm(common_recordings, desc=\"Creating windows\")):\n",
    "    # Get EEG data\n",
    "    eeg_rec = eeg_df[(eeg_df['subject'] == subject) & (eeg_df['activity_id'] == activity_id)]\n",
    "    eeg_data = eeg_rec[eeg_columns].values\n",
    "    \n",
    "    # Get ACG data\n",
    "    acg_rec = acg_df[(acg_df['subject'] == subject) & (acg_df['activity_id'] == activity_id)].sort_values('timestamp')\n",
    "    acg_data = acg_rec[['x', 'y', 'z']].values\n",
    "    \n",
    "    # Get GYRO data\n",
    "    gyro_rec = gyro_df[(gyro_df['subject'] == subject) & (gyro_df['activity_id'] == activity_id)].sort_values('timestamp')\n",
    "    gyro_data = gyro_rec[['x', 'y', 'z']].values\n",
    "    \n",
    "    # Calculate time duration for each\n",
    "    eeg_time = len(eeg_data) / EEG_FS\n",
    "    acg_time = len(acg_data) / HAR_FS\n",
    "    gyro_time = len(gyro_data) / HAR_FS\n",
    "    \n",
    "    # Use minimum time\n",
    "    min_time = min(eeg_time, acg_time, gyro_time)\n",
    "    \n",
    "    # Skip if too short\n",
    "    if min_time < WINDOW_SIZE_SEC:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    # Truncate all to min_time\n",
    "    eeg_samples = int(min_time * EEG_FS)\n",
    "    har_samples = int(min_time * HAR_FS)\n",
    "    \n",
    "    eeg_data = eeg_data[:eeg_samples]\n",
    "    acg_data = acg_data[:har_samples]\n",
    "    gyro_data = gyro_data[:har_samples]\n",
    "    \n",
    "    # Z-score normalize EEG\n",
    "    eeg_mean = np.mean(eeg_data, axis=0, keepdims=True)\n",
    "    eeg_std = np.std(eeg_data, axis=0, keepdims=True) + 1e-8\n",
    "    eeg_data = (eeg_data - eeg_mean) / eeg_std\n",
    "    \n",
    "    # Create windows (0% overlap)\n",
    "    n_windows = int(min_time // WINDOW_SIZE_SEC)\n",
    "    \n",
    "    for w_idx in range(n_windows):\n",
    "        # EEG window\n",
    "        eeg_start = w_idx * EEG_WINDOW_SAMPLES\n",
    "        eeg_end = eeg_start + EEG_WINDOW_SAMPLES\n",
    "        eeg_window = eeg_data[eeg_start:eeg_end, :]  # (500, 16)\n",
    "        \n",
    "        # HAR window (ACG + GYRO)\n",
    "        har_start = w_idx * HAR_WINDOW_SAMPLES\n",
    "        har_end = har_start + HAR_WINDOW_SAMPLES\n",
    "        acg_window = acg_data[har_start:har_end, :]  # (100, 3)\n",
    "        gyro_window = gyro_data[har_start:har_end, :]  # (100, 3)\n",
    "        \n",
    "        # Extract HAR features\n",
    "        har_features = extract_all_har_features(acg_window, gyro_window, HAR_FS)  # (152,)\n",
    "        \n",
    "        # Store\n",
    "        all_eeg_windows.append(eeg_window)\n",
    "        all_har_features.append(har_features)\n",
    "        all_labels.append(activity_id)\n",
    "        all_recording_ids.append(rec_idx)\n",
    "    \n",
    "    processed += 1\n",
    "\n",
    "print(f\"\\nProcessed: {processed} recordings\")\n",
    "print(f\"Skipped: {skipped} recordings (too short)\")\n",
    "\n",
    "# Convert to arrays\n",
    "X_eeg = np.array(all_eeg_windows)  # (N_windows, 500, 16)\n",
    "X_har_full = np.array(all_har_features)  # (N_windows, 152)\n",
    "y = np.array(all_labels)  # (N_windows,)\n",
    "recording_ids = np.array(all_recording_ids)  # (N_windows,)\n",
    "\n",
    "print(f\"\\nAligned dataset:\")\n",
    "print(f\"  EEG windows: {X_eeg.shape}\")\n",
    "print(f\"  HAR features (full): {X_har_full.shape}\")\n",
    "print(f\"  Labels: {y.shape}\")\n",
    "print(f\"  Unique labels: {np.unique(y)}\")\n",
    "\n",
    "# =====================================================\n",
    "# FEATURE SELECTION - CORRECT METHOD\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE SELECTION - SELECTING TOP 60 BY IMPORTANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load feature importance\n",
    "importance_df = pd.read_csv(IMPORTANCE_FILE)\n",
    "print(f\"\\nLoaded importance file: {importance_df.shape}\")\n",
    "\n",
    "# Get top 60 feature names\n",
    "top_60_feature_names = importance_df.head(60)['feature'].tolist()\n",
    "print(f\"\\nTop 10 most important features:\")\n",
    "for i, name in enumerate(top_60_feature_names[:10], 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "\n",
    "# Create index mapping\n",
    "print(f\"\\nMapping feature names to indices...\")\n",
    "top_60_indices = []\n",
    "missing_features = []\n",
    "\n",
    "for feat_name in top_60_feature_names:\n",
    "    if feat_name in all_feature_names:\n",
    "        idx = all_feature_names.index(feat_name)\n",
    "        top_60_indices.append(idx)\n",
    "    else:\n",
    "        missing_features.append(feat_name)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nWARNING: {len(missing_features)} features not found!\")\n",
    "    raise ValueError(\"Feature name mismatch - cannot proceed\")\n",
    "else:\n",
    "    print(f\"\\nSuccessfully mapped all 60 features!\")\n",
    "\n",
    "# Select the top 60 features\n",
    "X_har = X_har_full[:, top_60_indices]\n",
    "\n",
    "print(f\"\\nHAR features (top 60 by importance): {X_har.shape}\")\n",
    "\n",
    "# =====================================================\n",
    "# STEP 3: CREATE STRATIFIED 5-FOLD SPLIT\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: CREATE STRATIFIED 5-FOLD SPLIT (WINDOW-LEVEL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "fold_splits = []\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X_eeg, y)):\n",
    "    fold_splits.append({\n",
    "        'fold': fold_idx + 1,\n",
    "        'train_idx': train_idx,\n",
    "        'test_idx': test_idx\n",
    "    })\n",
    "    print(f\"  Fold {fold_idx + 1}: Train={len(train_idx)}, Test={len(test_idx)}\")\n",
    "\n",
    "print(f\"\\nNote: Using window-level stratified CV with 0% overlap and per-recording normalization\")\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS FOR METRICS\n",
    "# =====================================================\n",
    "\n",
    "def calculate_per_class_metrics(y_true, y_pred, y_pred_proba, class_names):\n",
    "    \"\"\"Calculate comprehensive per-class metrics\"\"\"\n",
    "    \n",
    "    # Per-class precision, recall, F1\n",
    "    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    # Per-class accuracy (fraction of correct predictions for each class)\n",
    "    accuracy_per_class = []\n",
    "    for class_id in range(len(class_names)):\n",
    "        class_mask = (y_true == class_id + 1)\n",
    "        if np.sum(class_mask) > 0:\n",
    "            class_acc = np.sum((y_pred == class_id + 1) & class_mask) / np.sum(class_mask)\n",
    "        else:\n",
    "            class_acc = 0.0\n",
    "        accuracy_per_class.append(class_acc)\n",
    "    \n",
    "    # Per-class AUC (one-vs-rest)\n",
    "    auc_per_class = []\n",
    "    for class_id in range(len(class_names)):\n",
    "        try:\n",
    "            y_true_binary = (y_true == class_id + 1).astype(int)\n",
    "            y_score = y_pred_proba[:, class_id]\n",
    "            if len(np.unique(y_true_binary)) > 1:  # Need both classes present\n",
    "                auc = roc_auc_score(y_true_binary, y_score)\n",
    "            else:\n",
    "                auc = 0.0\n",
    "        except:\n",
    "            auc = 0.0\n",
    "        auc_per_class.append(auc)\n",
    "    \n",
    "    # Create detailed dictionary\n",
    "    per_class_metrics = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        per_class_metrics[class_name] = {\n",
    "            'accuracy': float(accuracy_per_class[i]),\n",
    "            'precision': float(precision_per_class[i]),\n",
    "            'recall': float(recall_per_class[i]),\n",
    "            'f1': float(f1_per_class[i]),\n",
    "            'auc': float(auc_per_class[i])\n",
    "        }\n",
    "    \n",
    "    return per_class_metrics\n",
    "\n",
    "def print_per_class_metrics(per_class_metrics, title=\"PER-CLASS METRICS\"):\n",
    "    \"\"\"Pretty print per-class metrics\"\"\"\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(f\"{'Class':<30s} {'Acc':>8s} {'Prec':>8s} {'Rec':>8s} {'F1':>8s} {'AUC':>8s}\")\n",
    "    print(\"-\" * 75)\n",
    "    for class_name, metrics in per_class_metrics.items():\n",
    "        print(f\"{class_name:<30s} \"\n",
    "              f\"{metrics['accuracy']*100:>7.2f}% \"\n",
    "              f\"{metrics['precision']:>7.4f} \"\n",
    "              f\"{metrics['recall']:>7.4f} \"\n",
    "              f\"{metrics['f1']:>7.4f} \"\n",
    "              f\"{metrics['auc']:>7.4f}\")\n",
    "\n",
    "def compute_robustness_metrics(per_class_metrics):\n",
    "    \"\"\"Compute worst-task accuracy and cross-task variance\"\"\"\n",
    "    class_accuracies = [per_class_metrics[class_name]['accuracy'] for class_name in ACTIVITY_LABELS]\n",
    "    \n",
    "    worst_task_acc = min(class_accuracies)\n",
    "    best_task_acc = max(class_accuracies)\n",
    "    cross_task_variance = np.var(class_accuracies)\n",
    "    cross_task_std = np.std(class_accuracies)\n",
    "    \n",
    "    return {\n",
    "        'worst_task_accuracy': float(worst_task_acc),\n",
    "        'best_task_accuracy': float(best_task_acc),\n",
    "        'cross_task_variance': float(cross_task_variance),\n",
    "        'cross_task_std': float(cross_task_std)\n",
    "    }\n",
    "\n",
    "# =====================================================\n",
    "# STEP 4: TRAIN ALL 5 FOLDS\n",
    "# =====================================================\n",
    "\n",
    "# Store all results\n",
    "all_fold_results = []\n",
    "\n",
    "# EEGNet + Transformer model builder\n",
    "def build_eegnet_transformer():\n",
    "    class TransformerBlock(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "            super(TransformerBlock, self).__init__(**kwargs)\n",
    "            self.d_model = d_model\n",
    "            self.num_heads = num_heads\n",
    "            self.ff_dim = ff_dim\n",
    "            self.dropout_rate = dropout_rate\n",
    "            \n",
    "            self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads, dropout=dropout_rate)\n",
    "            self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(d_model)])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(dropout_rate)\n",
    "            self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        def call(self, inputs, training=False):\n",
    "            attn_output = self.att(inputs, inputs, training=training)\n",
    "            attn_output = self.dropout1(attn_output, training=training)\n",
    "            out1 = self.layernorm1(inputs + attn_output)\n",
    "            \n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            return self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({'d_model': self.d_model, 'num_heads': self.num_heads, \n",
    "                          'ff_dim': self.ff_dim, 'dropout_rate': self.dropout_rate})\n",
    "            return config\n",
    "\n",
    "    class PositionalEncoding(layers.Layer):\n",
    "        def __init__(self, sequence_length, d_model, **kwargs):\n",
    "            super(PositionalEncoding, self).__init__(**kwargs)\n",
    "            self.sequence_length = sequence_length\n",
    "            self.d_model = d_model\n",
    "            self.pos_encoding = self.add_weight(name='pos_encoding', shape=(1, sequence_length, d_model),\n",
    "                                               initializer='zeros', trainable=True)\n",
    "        \n",
    "        def call(self, x):\n",
    "            return x + self.pos_encoding\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({'sequence_length': self.sequence_length, 'd_model': self.d_model})\n",
    "            return config\n",
    "\n",
    "    F1, D, F2 = 8, 2, 16\n",
    "    kernel_length = 64\n",
    "\n",
    "    input_layer = layers.Input(shape=(EEG_WINDOW_SAMPLES, EEG_CHANNELS, 1))\n",
    "\n",
    "    block1 = layers.Conv2D(F1, (kernel_length, 1), padding='same', use_bias=False)(input_layer)\n",
    "    block1 = layers.BatchNormalization()(block1)\n",
    "    block1 = layers.DepthwiseConv2D((1, EEG_CHANNELS), use_bias=False, depth_multiplier=D,\n",
    "                                   depthwise_constraint=keras.constraints.max_norm(1.))(block1)\n",
    "    block1 = layers.BatchNormalization()(block1)\n",
    "    block1 = layers.Activation('elu')(block1)\n",
    "    block1 = layers.AveragePooling2D((4, 1))(block1)\n",
    "    block1 = layers.Dropout(EEGNET_DROPOUT)(block1)\n",
    "\n",
    "    block2 = layers.SeparableConv2D(F2, (16, 1), use_bias=False, padding='same')(block1)\n",
    "    block2 = layers.BatchNormalization()(block2)\n",
    "    block2 = layers.Activation('elu')(block2)\n",
    "    block2 = layers.AveragePooling2D((8, 1))(block2)\n",
    "    block2 = layers.Dropout(EEGNET_DROPOUT)(block2)\n",
    "\n",
    "    reshape = layers.Reshape((15, 16))(block2)\n",
    "    projection = layers.Dense(D_MODEL)(reshape)\n",
    "    pos_encoding = PositionalEncoding(sequence_length=15, d_model=D_MODEL)(projection)\n",
    "\n",
    "    x = pos_encoding\n",
    "    for i in range(NUM_LAYERS):\n",
    "        x = TransformerBlock(d_model=D_MODEL, num_heads=NUM_HEADS, ff_dim=FF_DIM,\n",
    "                            dropout_rate=TRANSFORMER_DROPOUT, name=f'transformer_block_{i+1}')(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(TRANSFORMER_DROPOUT)(x)\n",
    "    dense = layers.Dense(N_CLASSES, kernel_constraint=keras.constraints.max_norm(0.25))(x)\n",
    "    softmax = layers.Activation('softmax')(dense)\n",
    "\n",
    "    model = models.Model(inputs=input_layer, outputs=softmax)\n",
    "    \n",
    "    return model\n",
    "\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"FOLD {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fold_data = fold_splits[fold_idx]\n",
    "    train_idx = fold_data['train_idx']\n",
    "    test_idx = fold_data['test_idx']\n",
    "    \n",
    "    # Split train into train/val (80/20)\n",
    "    train_idx_actual, val_idx = train_test_split(\n",
    "        train_idx,\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=y[train_idx]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Train: {len(train_idx_actual)} windows\")\n",
    "    print(f\"  Val:   {len(val_idx)} windows\")\n",
    "    print(f\"  Test:  {len(test_idx)} windows\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # EEG BRANCH: EEGNet + Transformer\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(\"EEG BRANCH: EEGNET + TRANSFORMER\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_eeg_train = X_eeg[train_idx_actual]\n",
    "    X_eeg_val = X_eeg[val_idx]\n",
    "    X_eeg_test = X_eeg[test_idx]\n",
    "    \n",
    "    y_eeg_train = y[train_idx_actual]\n",
    "    y_eeg_val = y[val_idx]\n",
    "    y_eeg_test = y[test_idx]\n",
    "    \n",
    "    # Reshape for model\n",
    "    X_eeg_train_reshaped = X_eeg_train.reshape(len(X_eeg_train), EEG_WINDOW_SAMPLES, EEG_CHANNELS, 1)\n",
    "    X_eeg_val_reshaped = X_eeg_val.reshape(len(X_eeg_val), EEG_WINDOW_SAMPLES, EEG_CHANNELS, 1)\n",
    "    X_eeg_test_reshaped = X_eeg_test.reshape(len(X_eeg_test), EEG_WINDOW_SAMPLES, EEG_CHANNELS, 1)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    y_eeg_train_cat = to_categorical(y_eeg_train - 1, N_CLASSES)\n",
    "    y_eeg_val_cat = to_categorical(y_eeg_val - 1, N_CLASSES)\n",
    "    \n",
    "    # Build model\n",
    "    print(\"\\nBuilding EEGNet + Transformer...\")\n",
    "    eeg_model = build_eegnet_transformer()\n",
    "    \n",
    "    # Compile\n",
    "    eeg_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_eeg_train), y=y_eeg_train)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=0)\n",
    "    \n",
    "    # Train\n",
    "    print(\"Training EEG model...\")\n",
    "    eeg_history = eeg_model.fit(\n",
    "        X_eeg_train_reshaped, y_eeg_train_cat,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_eeg_val_reshaped, y_eeg_val_cat),\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Training complete! Epochs: {len(eeg_history.history['loss'])}\")\n",
    "    \n",
    "    # Predict on test\n",
    "    print(\"Evaluating EEG on test set...\")\n",
    "    eeg_test_probs = eeg_model.predict(X_eeg_test_reshaped, verbose=0)\n",
    "    eeg_test_preds = np.argmax(eeg_test_probs, axis=1) + 1\n",
    "    \n",
    "    eeg_test_acc = accuracy_score(y_eeg_test, eeg_test_preds)\n",
    "    eeg_test_f1 = f1_score(y_eeg_test, eeg_test_preds, average='macro')\n",
    "    \n",
    "    print(f\"EEG Test Results: Accuracy={eeg_test_acc*100:.2f}%, F1={eeg_test_f1:.4f}\")\n",
    "    \n",
    "    # Calculate per-class metrics for EEG\n",
    "    eeg_per_class = calculate_per_class_metrics(y_eeg_test, eeg_test_preds, eeg_test_probs, ACTIVITY_LABELS)\n",
    "    print_per_class_metrics(eeg_per_class, \"EEG PER-CLASS METRICS\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # HAR BRANCH: XGBoost\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(\"HAR BRANCH: XGBOOST\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_har_train = X_har[train_idx_actual]\n",
    "    X_har_val = X_har[val_idx]\n",
    "    X_har_test = X_har[test_idx]\n",
    "    \n",
    "    y_har_train = y[train_idx_actual]\n",
    "    y_har_val = y[val_idx]\n",
    "    y_har_test = y[test_idx]\n",
    "    \n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    X_har_train_scaled = scaler.fit_transform(X_har_train)\n",
    "    X_har_val_scaled = scaler.transform(X_har_val)\n",
    "    X_har_test_scaled = scaler.transform(X_har_test)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print(\"Training HAR model...\")\n",
    "    har_model = xgb.XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=N_CLASSES,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_SEED,\n",
    "        tree_method='gpu_hist',\n",
    "        gpu_id=0,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "    har_model.fit(X_har_train_scaled, y_har_train - 1, verbose=False)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Predict on test\n",
    "    print(\"Evaluating HAR on test set...\")\n",
    "    har_test_probs = har_model.predict_proba(X_har_test_scaled)\n",
    "    har_test_preds = har_model.predict(X_har_test_scaled) + 1 \n",
    "    \n",
    "    har_test_acc = accuracy_score(y_har_test, har_test_preds)\n",
    "    har_test_f1 = f1_score(y_har_test, har_test_preds, average='macro')\n",
    "    \n",
    "    print(f\"HAR Test Results: Accuracy={har_test_acc*100:.2f}%, F1={har_test_f1:.4f}\")\n",
    "    \n",
    "    # Calculate per-class metrics for HAR\n",
    "    har_per_class = calculate_per_class_metrics(y_har_test, har_test_preds, har_test_probs, ACTIVITY_LABELS)\n",
    "    print_per_class_metrics(har_per_class, \"HAR PER-CLASS METRICS\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # FUSION\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(\"FUSION METHODS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Method 1: Simple Averaging\n",
    "    print(\"\\n1. Simple Averaging (50-50)...\")\n",
    "    simple_avg_probs = 0.5 * eeg_test_probs + 0.5 * har_test_probs\n",
    "    simple_avg_preds = np.argmax(simple_avg_probs, axis=1) + 1\n",
    "    simple_avg_acc = accuracy_score(y_eeg_test, simple_avg_preds)\n",
    "    simple_avg_f1 = f1_score(y_eeg_test, simple_avg_preds, average='macro')\n",
    "    \n",
    "    print(f\"   Accuracy: {simple_avg_acc*100:.2f}%, F1: {simple_avg_f1:.4f}\")\n",
    "    simple_avg_per_class = calculate_per_class_metrics(y_eeg_test, simple_avg_preds, simple_avg_probs, ACTIVITY_LABELS)\n",
    "    \n",
    "    # Method 2: Weighted Averaging\n",
    "    print(\"\\n2. Weighted Averaging (EEG: 0.503, HAR: 0.497)...\")\n",
    "    weight_eeg = 0.503\n",
    "    weight_har = 0.497\n",
    "    weighted_avg_probs = weight_eeg * eeg_test_probs + weight_har * har_test_probs\n",
    "    weighted_avg_preds = np.argmax(weighted_avg_probs, axis=1) + 1\n",
    "    weighted_avg_acc = accuracy_score(y_eeg_test, weighted_avg_preds)\n",
    "    weighted_avg_f1 = f1_score(y_eeg_test, weighted_avg_preds, average='macro')\n",
    "    \n",
    "    print(f\"   Accuracy: {weighted_avg_acc*100:.2f}%, F1: {weighted_avg_f1:.4f}\")\n",
    "    weighted_avg_per_class = calculate_per_class_metrics(y_eeg_test, weighted_avg_preds, weighted_avg_probs, ACTIVITY_LABELS)\n",
    "    \n",
    "    # Method 3: Logistic Regression\n",
    "    print(\"\\n3. Logistic Regression...\")\n",
    "    eeg_val_probs = eeg_model.predict(X_eeg_val_reshaped, verbose=0)\n",
    "    har_val_probs = har_model.predict_proba(X_har_val_scaled)\n",
    "    \n",
    "    X_val_fusion = np.hstack([eeg_val_probs, har_val_probs])\n",
    "    X_test_fusion = np.hstack([eeg_test_probs, har_test_probs])\n",
    "    \n",
    "    lr_fusion = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
    "    lr_fusion.fit(X_val_fusion, y_eeg_val)\n",
    "    \n",
    "    lr_preds = lr_fusion.predict(X_test_fusion)\n",
    "    lr_probs = lr_fusion.predict_proba(X_test_fusion)\n",
    "    lr_acc = accuracy_score(y_eeg_test, lr_preds)\n",
    "    lr_f1 = f1_score(y_eeg_test, lr_preds, average='macro')\n",
    "    \n",
    "    print(f\"   Accuracy: {lr_acc*100:.2f}%, F1: {lr_f1:.4f}\")\n",
    "    lr_per_class = calculate_per_class_metrics(y_eeg_test, lr_preds, lr_probs, ACTIVITY_LABELS)\n",
    "    \n",
    "    # Method 4: MLP\n",
    "    print(\"\\n4. MLP Fusion...\")\n",
    "    mlp_fusion = MLPClassifier(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=500,\n",
    "        random_state=RANDOM_SEED,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    mlp_fusion.fit(X_val_fusion, y_eeg_val)\n",
    "    \n",
    "    mlp_preds = mlp_fusion.predict(X_test_fusion)\n",
    "    mlp_probs = mlp_fusion.predict_proba(X_test_fusion)\n",
    "    mlp_acc = accuracy_score(y_eeg_test, mlp_preds)\n",
    "    mlp_f1 = f1_score(y_eeg_test, mlp_preds, average='macro')\n",
    "    \n",
    "    print(f\"   Accuracy: {mlp_acc*100:.2f}%, F1: {mlp_f1:.4f}\")\n",
    "    mlp_per_class = calculate_per_class_metrics(y_eeg_test, mlp_preds, mlp_probs, ACTIVITY_LABELS)\n",
    "    \n",
    "    # ==========================================\n",
    "    # COMPUTE ROBUSTNESS METRICS FOR THIS FOLD\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*70)\n",
    "    print(\"ROBUSTNESS METRICS\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Compute for each method\n",
    "    eeg_robustness = compute_robustness_metrics(eeg_per_class)\n",
    "    har_robustness = compute_robustness_metrics(har_per_class)\n",
    "    simple_avg_robustness = compute_robustness_metrics(simple_avg_per_class)\n",
    "    weighted_avg_robustness = compute_robustness_metrics(weighted_avg_per_class)\n",
    "    lr_robustness = compute_robustness_metrics(lr_per_class)\n",
    "    mlp_robustness = compute_robustness_metrics(mlp_per_class)\n",
    "    \n",
    "    # Print robustness metrics\n",
    "    print(f\"\\n{'Method':<25s} {'Worst-Task':>12s} {'Best-Task':>12s} {'Cross-Task Std':>15s}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'EEG only':<25s} {eeg_robustness['worst_task_accuracy']*100:>11.2f}% {eeg_robustness['best_task_accuracy']*100:>11.2f}% {eeg_robustness['cross_task_std']*100:>14.2f}%\")\n",
    "    print(f\"{'HAR only':<25s} {har_robustness['worst_task_accuracy']*100:>11.2f}% {har_robustness['best_task_accuracy']*100:>11.2f}% {har_robustness['cross_task_std']*100:>14.2f}%\")\n",
    "    print(f\"{'Simple Averaging':<25s} {simple_avg_robustness['worst_task_accuracy']*100:>11.2f}% {simple_avg_robustness['best_task_accuracy']*100:>11.2f}% {simple_avg_robustness['cross_task_std']*100:>14.2f}%\")\n",
    "    print(f\"{'Weighted Averaging':<25s} {weighted_avg_robustness['worst_task_accuracy']*100:>11.2f}% {weighted_avg_robustness['best_task_accuracy']*100:>11.2f}% {weighted_avg_robustness['cross_task_std']*100:>14.2f}%\")\n",
    "    print(f\"{'Logistic Regression':<25s} {lr_robustness['worst_task_accuracy']*100:>11.2f}% {lr_robustness['best_task_accuracy']*100:>11.2f}% {lr_robustness['cross_task_std']*100:>14.2f}%\")\n",
    "    print(f\"{'MLP':<25s} {mlp_robustness['worst_task_accuracy']*100:>11.2f}% {mlp_robustness['best_task_accuracy']*100:>11.2f}% {mlp_robustness['cross_task_std']*100:>14.2f}%\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # FOLD SUMMARY\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"FOLD {fold_idx + 1} SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n{'Method':<25s} {'Accuracy':>12s} {'F1 (macro)':>12s}\")\n",
    "    print(\"-\" * 52)\n",
    "    print(f\"{'EEG only':<25s} {eeg_test_acc*100:>11.2f}% {eeg_test_f1:>12.4f}\")\n",
    "    print(f\"{'HAR only':<25s} {har_test_acc*100:>11.2f}% {har_test_f1:>12.4f}\")\n",
    "    print(f\"{'Simple Averaging':<25s} {simple_avg_acc*100:>11.2f}% {simple_avg_f1:>12.4f}\")\n",
    "    print(f\"{'Weighted Averaging':<25s} {weighted_avg_acc*100:>11.2f}% {weighted_avg_f1:>12.4f}\")\n",
    "    print(f\"{'Logistic Regression':<25s} {lr_acc*100:>11.2f}% {lr_f1:>12.4f}\")\n",
    "    print(f\"{'MLP':<25s} {mlp_acc*100:>11.2f}% {mlp_f1:>12.4f}\")\n",
    "    \n",
    "    # Store fold results WITH robustness metrics\n",
    "    fold_results = {\n",
    "        'fold': fold_idx + 1,\n",
    "        'eeg': {\n",
    "            'accuracy': float(eeg_test_acc),\n",
    "            'f1_macro': float(eeg_test_f1),\n",
    "            'per_class': eeg_per_class,\n",
    "            'worst_task_accuracy': eeg_robustness['worst_task_accuracy'],\n",
    "            'best_task_accuracy': eeg_robustness['best_task_accuracy'],\n",
    "            'cross_task_variance': eeg_robustness['cross_task_variance'],\n",
    "            'cross_task_std': eeg_robustness['cross_task_std']\n",
    "        },\n",
    "        'har': {\n",
    "            'accuracy': float(har_test_acc),\n",
    "            'f1_macro': float(har_test_f1),\n",
    "            'per_class': har_per_class,\n",
    "            'worst_task_accuracy': har_robustness['worst_task_accuracy'],\n",
    "            'best_task_accuracy': har_robustness['best_task_accuracy'],\n",
    "            'cross_task_variance': har_robustness['cross_task_variance'],\n",
    "            'cross_task_std': har_robustness['cross_task_std']\n",
    "        },\n",
    "        'simple_averaging': {\n",
    "            'accuracy': float(simple_avg_acc),\n",
    "            'f1_macro': float(simple_avg_f1),\n",
    "            'per_class': simple_avg_per_class,\n",
    "            'worst_task_accuracy': simple_avg_robustness['worst_task_accuracy'],\n",
    "            'best_task_accuracy': simple_avg_robustness['best_task_accuracy'],\n",
    "            'cross_task_variance': simple_avg_robustness['cross_task_variance'],\n",
    "            'cross_task_std': simple_avg_robustness['cross_task_std']\n",
    "        },\n",
    "        'weighted_averaging': {\n",
    "            'accuracy': float(weighted_avg_acc),\n",
    "            'f1_macro': float(weighted_avg_f1),\n",
    "            'per_class': weighted_avg_per_class,\n",
    "            'worst_task_accuracy': weighted_avg_robustness['worst_task_accuracy'],\n",
    "            'best_task_accuracy': weighted_avg_robustness['best_task_accuracy'],\n",
    "            'cross_task_variance': weighted_avg_robustness['cross_task_variance'],\n",
    "            'cross_task_std': weighted_avg_robustness['cross_task_std']\n",
    "        },\n",
    "        'logistic_regression': {\n",
    "            'accuracy': float(lr_acc),\n",
    "            'f1_macro': float(lr_f1),\n",
    "            'per_class': lr_per_class,\n",
    "            'worst_task_accuracy': lr_robustness['worst_task_accuracy'],\n",
    "            'best_task_accuracy': lr_robustness['best_task_accuracy'],\n",
    "            'cross_task_variance': lr_robustness['cross_task_variance'],\n",
    "            'cross_task_std': lr_robustness['cross_task_std']\n",
    "        },\n",
    "        'mlp': {\n",
    "            'accuracy': float(mlp_acc),\n",
    "            'f1_macro': float(mlp_f1),\n",
    "            'per_class': mlp_per_class,\n",
    "            'worst_task_accuracy': mlp_robustness['worst_task_accuracy'],\n",
    "            'best_task_accuracy': mlp_robustness['best_task_accuracy'],\n",
    "            'cross_task_variance': mlp_robustness['cross_task_variance'],\n",
    "            'cross_task_std': mlp_robustness['cross_task_std']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_fold_results.append(fold_results)\n",
    "    \n",
    "    # Save fold results\n",
    "    fold_dir = os.path.join(OUTPUT_DIR, f'fold_{fold_idx + 1}')\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(fold_dir, 'results.json'), 'w') as f:\n",
    "        json.dump(fold_results, f, indent=2)\n",
    "    \n",
    "    # Save confusion matrices\n",
    "    np.save(os.path.join(fold_dir, 'cm_eeg.npy'), confusion_matrix(y_eeg_test, eeg_test_preds))\n",
    "    np.save(os.path.join(fold_dir, 'cm_har.npy'), confusion_matrix(y_har_test, har_test_preds))\n",
    "    np.save(os.path.join(fold_dir, 'cm_simple_avg.npy'), confusion_matrix(y_eeg_test, simple_avg_preds))\n",
    "    np.save(os.path.join(fold_dir, 'cm_weighted_avg.npy'), confusion_matrix(y_eeg_test, weighted_avg_preds))\n",
    "    np.save(os.path.join(fold_dir, 'cm_lr.npy'), confusion_matrix(y_eeg_test, lr_preds))\n",
    "    np.save(os.path.join(fold_dir, 'cm_mlp.npy'), confusion_matrix(y_eeg_test, mlp_preds))\n",
    "    \n",
    "    # Clear models to free memory\n",
    "    del eeg_model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "# =====================================================\n",
    "# AGGREGATE RESULTS ACROSS ALL FOLDS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS - AGGREGATE ACROSS ALL 5 FOLDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract metrics for each method\n",
    "methods = ['eeg', 'har', 'simple_averaging', 'weighted_averaging', 'logistic_regression', 'mlp']\n",
    "method_names = {\n",
    "    'eeg': 'EEG only',\n",
    "    'har': 'HAR only',\n",
    "    'simple_averaging': 'Simple Averaging',\n",
    "    'weighted_averaging': 'Weighted Averaging',\n",
    "    'logistic_regression': 'Logistic Regression',\n",
    "    'mlp': 'MLP'\n",
    "}\n",
    "\n",
    "aggregate_results = {}\n",
    "\n",
    "for method in methods:\n",
    "    accuracies = [r[method]['accuracy'] for r in all_fold_results]\n",
    "    f1_scores = [r[method]['f1_macro'] for r in all_fold_results]\n",
    "    \n",
    "    # Collect robustness metrics\n",
    "    worst_task_accs = [r[method]['worst_task_accuracy'] for r in all_fold_results]\n",
    "    best_task_accs = [r[method]['best_task_accuracy'] for r in all_fold_results]\n",
    "    cross_task_variances = [r[method]['cross_task_variance'] for r in all_fold_results]\n",
    "    cross_task_stds = [r[method]['cross_task_std'] for r in all_fold_results]\n",
    "    \n",
    "    # Aggregate per-class metrics\n",
    "    per_class_aggregate = {}\n",
    "    for class_name in ACTIVITY_LABELS:\n",
    "        class_accuracies = [r[method]['per_class'][class_name]['accuracy'] for r in all_fold_results]\n",
    "        class_precisions = [r[method]['per_class'][class_name]['precision'] for r in all_fold_results]\n",
    "        class_recalls = [r[method]['per_class'][class_name]['recall'] for r in all_fold_results]\n",
    "        class_f1s = [r[method]['per_class'][class_name]['f1'] for r in all_fold_results]\n",
    "        class_aucs = [r[method]['per_class'][class_name]['auc'] for r in all_fold_results]\n",
    "        \n",
    "        per_class_aggregate[class_name] = {\n",
    "            'accuracy_mean': float(np.mean(class_accuracies)),\n",
    "            'accuracy_std': float(np.std(class_accuracies)),\n",
    "            'precision_mean': float(np.mean(class_precisions)),\n",
    "            'precision_std': float(np.std(class_precisions)),\n",
    "            'recall_mean': float(np.mean(class_recalls)),\n",
    "            'recall_std': float(np.std(class_recalls)),\n",
    "            'f1_mean': float(np.mean(class_f1s)),\n",
    "            'f1_std': float(np.std(class_f1s)),\n",
    "            'auc_mean': float(np.mean(class_aucs)),\n",
    "            'auc_std': float(np.std(class_aucs))\n",
    "        }\n",
    "    \n",
    "    aggregate_results[method] = {\n",
    "        'accuracy_mean': float(np.mean(accuracies)),\n",
    "        'accuracy_std': float(np.std(accuracies)),\n",
    "        'f1_macro_mean': float(np.mean(f1_scores)),\n",
    "        'f1_macro_std': float(np.std(f1_scores)),\n",
    "        'worst_task_accuracy_mean': float(np.mean(worst_task_accs)),\n",
    "        'worst_task_accuracy_std': float(np.std(worst_task_accs)),\n",
    "        'best_task_accuracy_mean': float(np.mean(best_task_accs)),\n",
    "        'best_task_accuracy_std': float(np.std(best_task_accs)),\n",
    "        'cross_task_variance_mean': float(np.mean(cross_task_variances)),\n",
    "        'cross_task_variance_std': float(np.std(cross_task_variances)),\n",
    "        'cross_task_std_mean': float(np.mean(cross_task_stds)),\n",
    "        'cross_task_std_std': float(np.std(cross_task_stds)),\n",
    "        'per_class': per_class_aggregate\n",
    "    }\n",
    "\n",
    "# =====================================================\n",
    "# STATISTICAL TESTS: PAIRED T-TESTS ACROSS FOLDS\n",
    "# =====================================================\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS (PAIRED T-TESTS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Collect fold-level metrics for each method\n",
    "method_accuracies = {method: [] for method in methods}\n",
    "method_f1s = {method: [] for method in methods}\n",
    "method_worst_task = {method: [] for method in methods}\n",
    "\n",
    "for fold_result in all_fold_results:\n",
    "    for method in methods:\n",
    "        method_accuracies[method].append(fold_result[method]['accuracy'])\n",
    "        method_f1s[method].append(fold_result[method]['f1_macro'])\n",
    "        method_worst_task[method].append(fold_result[method]['worst_task_accuracy'])\n",
    "\n",
    "# Convert to arrays\n",
    "for method in methods:\n",
    "    method_accuracies[method] = np.array(method_accuracies[method])\n",
    "    method_f1s[method] = np.array(method_f1s[method])\n",
    "    method_worst_task[method] = np.array(method_worst_task[method])\n",
    "\n",
    "# Test 1: Fusion (Logistic Regression) vs EEG\n",
    "print(\"\\n1. Logistic Regression Fusion vs EEG-only:\")\n",
    "t_stat_acc, p_value_acc = ttest_rel(method_accuracies['logistic_regression'], method_accuracies['eeg'])\n",
    "t_stat_f1, p_value_f1 = ttest_rel(method_f1s['logistic_regression'], method_f1s['eeg'])\n",
    "t_stat_worst, p_value_worst = ttest_rel(method_worst_task['logistic_regression'], method_worst_task['eeg'])\n",
    "\n",
    "print(f\"   Overall Accuracy:  t={t_stat_acc:.4f}, p={p_value_acc:.6f} {'***' if p_value_acc < 0.001 else '**' if p_value_acc < 0.01 else '*' if p_value_acc < 0.05 else 'ns'}\")\n",
    "print(f\"   Macro F1-score:    t={t_stat_f1:.4f}, p={p_value_f1:.6f} {'***' if p_value_f1 < 0.001 else '**' if p_value_f1 < 0.01 else '*' if p_value_f1 < 0.05 else 'ns'}\")\n",
    "print(f\"   Worst-task Acc:    t={t_stat_worst:.4f}, p={p_value_worst:.6f} {'***' if p_value_worst < 0.001 else '**' if p_value_worst < 0.01 else '*' if p_value_worst < 0.05 else 'ns'}\")\n",
    "\n",
    "# Test 2: Fusion (Logistic Regression) vs HAR\n",
    "print(\"\\n2. Logistic Regression Fusion vs HAR-only:\")\n",
    "t_stat_acc2, p_value_acc2 = ttest_rel(method_accuracies['logistic_regression'], method_accuracies['har'])\n",
    "t_stat_f12, p_value_f12 = ttest_rel(method_f1s['logistic_regression'], method_f1s['har'])\n",
    "t_stat_worst2, p_value_worst2 = ttest_rel(method_worst_task['logistic_regression'], method_worst_task['har'])\n",
    "\n",
    "print(f\"   Overall Accuracy:  t={t_stat_acc2:.4f}, p={p_value_acc2:.6f} {'***' if p_value_acc2 < 0.001 else '**' if p_value_acc2 < 0.01 else '*' if p_value_acc2 < 0.05 else 'ns'}\")\n",
    "print(f\"   Macro F1-score:    t={t_stat_f12:.4f}, p={p_value_f12:.6f} {'***' if p_value_f12 < 0.001 else '**' if p_value_f12 < 0.01 else '*' if p_value_f12 < 0.05 else 'ns'}\")\n",
    "print(f\"   Worst-task Acc:    t={t_stat_worst2:.4f}, p={p_value_worst2:.6f} {'***' if p_value_worst2 < 0.001 else '**' if p_value_worst2 < 0.01 else '*' if p_value_worst2 < 0.05 else 'ns'}\")\n",
    "\n",
    "# Test 3: Fusion vs Best Single Modality (per fold)\n",
    "print(\"\\n3. Logistic Regression Fusion vs Best Single Modality (per fold):\")\n",
    "best_single_modality_per_fold = []\n",
    "for i in range(N_FOLDS):\n",
    "    best_acc = max(method_accuracies['eeg'][i], method_accuracies['har'][i])\n",
    "    best_single_modality_per_fold.append(best_acc)\n",
    "\n",
    "best_single_modality_per_fold = np.array(best_single_modality_per_fold)\n",
    "t_stat3, p_value3 = ttest_rel(method_accuracies['logistic_regression'], best_single_modality_per_fold)\n",
    "\n",
    "print(f\"   Overall Accuracy:  t={t_stat3:.4f}, p={p_value3:.6f} {'***' if p_value3 < 0.001 else '**' if p_value3 < 0.01 else '*' if p_value3 < 0.05 else 'ns'}\")\n",
    "\n",
    "# Store statistical test results\n",
    "statistical_tests = {\n",
    "    'fusion_vs_eeg': {\n",
    "        'accuracy': {'t_statistic': float(t_stat_acc), 'p_value': float(p_value_acc)},\n",
    "        'f1_macro': {'t_statistic': float(t_stat_f1), 'p_value': float(p_value_f1)},\n",
    "        'worst_task': {'t_statistic': float(t_stat_worst), 'p_value': float(p_value_worst)}\n",
    "    },\n",
    "    'fusion_vs_har': {\n",
    "        'accuracy': {'t_statistic': float(t_stat_acc2), 'p_value': float(p_value_acc2)},\n",
    "        'f1_macro': {'t_statistic': float(t_stat_f12), 'p_value': float(p_value_f12)},\n",
    "        'worst_task': {'t_statistic': float(t_stat_worst2), 'p_value': float(p_value_worst2)}\n",
    "    },\n",
    "    'fusion_vs_best_single': {\n",
    "        'accuracy': {'t_statistic': float(t_stat3), 'p_value': float(p_value3)}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n Statistical significance tests complete\")\n",
    "print(f\"  (* p<0.05, ** p<0.01, *** p<0.001, ns = not significant)\")\n",
    "\n",
    "# Print overall summary\n",
    "print(f\"\\n{'Method':<25s} {'Accuracy':>20s} {'F1 (macro)':>20s} {'Worst-Task Acc':>20s}\")\n",
    "print(\"-\" * 90)\n",
    "for method in methods:\n",
    "    acc_mean = aggregate_results[method]['accuracy_mean']\n",
    "    acc_std = aggregate_results[method]['accuracy_std']\n",
    "    f1_mean = aggregate_results[method]['f1_macro_mean']\n",
    "    f1_std = aggregate_results[method]['f1_macro_std']\n",
    "    worst_mean = aggregate_results[method]['worst_task_accuracy_mean']\n",
    "    worst_std = aggregate_results[method]['worst_task_accuracy_std']\n",
    "    \n",
    "    print(f\"{method_names[method]:<25s} \"\n",
    "          f\"{acc_mean*100:>6.2f}% ± {acc_std*100:>4.2f}%  \"\n",
    "          f\"{f1_mean:>6.4f} ± {f1_std:>5.4f}  \"\n",
    "          f\"{worst_mean*100:>6.2f}% ± {worst_std*100:>4.2f}%\")\n",
    "\n",
    "# Add robustness metrics summary\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUSTNESS METRICS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Method':<25s} {'Worst-Task Acc':>20s} {'Cross-Task Std':>20s}\")\n",
    "print(\"-\" * 68)\n",
    "for method in methods:\n",
    "    worst_mean = aggregate_results[method]['worst_task_accuracy_mean']\n",
    "    worst_std = aggregate_results[method]['worst_task_accuracy_std']\n",
    "    std_mean = aggregate_results[method]['cross_task_std_mean']\n",
    "    std_std = aggregate_results[method]['cross_task_std_std']\n",
    "    \n",
    "    print(f\"{method_names[method]:<25s} \"\n",
    "          f\"{worst_mean*100:>6.2f}% ± {worst_std*100:>4.2f}%  \"\n",
    "          f\"{std_mean*100:>6.2f}% ± {std_std*100:>4.2f}%\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - Worst-task accuracy: Minimum per-class accuracy (measures coverage)\")\n",
    "print(f\"  - Cross-task std: Standard deviation of per-class accuracies (measures consistency)\")\n",
    "print(f\"  - Lower cross-task std = more consistent across activities\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PER-CLASS RESULTS FOR EEG-ONLY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Class':<30s} {'Acc':>12s} {'Prec':>12s} {'Rec':>12s} {'F1':>12s} {'AUC':>12s}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for class_name in ACTIVITY_LABELS:\n",
    "    metrics = aggregate_results['eeg']['per_class'][class_name]\n",
    "    print(f\"{class_name:<30s} \"\n",
    "          f\"{metrics['accuracy_mean']*100:>5.2f}±{metrics['accuracy_std']*100:>4.2f}% \"\n",
    "          f\"{metrics['precision_mean']:>5.3f}±{metrics['precision_std']:>4.3f} \"\n",
    "          f\"{metrics['recall_mean']:>5.3f}±{metrics['recall_std']:>4.3f} \"\n",
    "          f\"{metrics['f1_mean']:>5.3f}±{metrics['f1_std']:>4.3f} \"\n",
    "          f\"{metrics['auc_mean']:>5.3f}±{metrics['auc_std']:>4.3f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"PER-CLASS RESULTS FOR HAR-ONLY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Class':<30s} {'Acc':>12s} {'Prec':>12s} {'Rec':>12s} {'F1':>12s} {'AUC':>12s}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for class_name in ACTIVITY_LABELS:\n",
    "    metrics = aggregate_results['har']['per_class'][class_name]\n",
    "    print(f\"{class_name:<30s} \"\n",
    "          f\"{metrics['accuracy_mean']*100:>5.2f}±{metrics['accuracy_std']*100:>4.2f}% \"\n",
    "          f\"{metrics['precision_mean']:>5.3f}±{metrics['precision_std']:>4.3f} \"\n",
    "          f\"{metrics['recall_mean']:>5.3f}±{metrics['recall_std']:>4.3f} \"\n",
    "          f\"{metrics['f1_mean']:>5.3f}±{metrics['f1_std']:>4.3f} \"\n",
    "          f\"{metrics['auc_mean']:>5.3f}±{metrics['auc_std']:>4.3f}\")    \n",
    "    \n",
    "# Find best method\n",
    "best_method = max(methods, key=lambda m: aggregate_results[m]['accuracy_mean'])\n",
    "print(f\"\\nBest method: {method_names[best_method]} \"\n",
    "      f\"({aggregate_results[best_method]['accuracy_mean']*100:.2f}% ± \"\n",
    "      f\"{aggregate_results[best_method]['accuracy_std']*100:.2f}%)\")\n",
    "\n",
    "# Print per-class aggregate for best method\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"PER-CLASS RESULTS FOR BEST METHOD: {method_names[best_method].upper()}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Class':<30s} {'Acc':>12s} {'Prec':>12s} {'Rec':>12s} {'F1':>12s} {'AUC':>12s}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for class_name in ACTIVITY_LABELS:\n",
    "    metrics = aggregate_results[best_method]['per_class'][class_name]\n",
    "    print(f\"{class_name:<30s} \"\n",
    "          f\"{metrics['accuracy_mean']*100:>5.2f}±{metrics['accuracy_std']*100:>4.2f}% \"\n",
    "          f\"{metrics['precision_mean']:>5.3f}±{metrics['precision_std']:>4.3f} \"\n",
    "          f\"{metrics['recall_mean']:>5.3f}±{metrics['recall_std']:>4.3f} \"\n",
    "          f\"{metrics['f1_mean']:>5.3f}±{metrics['f1_std']:>4.3f} \"\n",
    "          f\"{metrics['auc_mean']:>5.3f}±{metrics['auc_std']:>4.3f}\")\n",
    "\n",
    "# Save final aggregate results\n",
    "final_results = {\n",
    "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'experiment': 'Unified_Multimodal_EEG_HAR_Fusion_5Fold',\n",
    "    'configuration': {\n",
    "        'window_size_sec': WINDOW_SIZE_SEC,\n",
    "        'overlap': OVERLAP,\n",
    "        'eeg_fs': EEG_FS,\n",
    "        'har_fs': HAR_FS,\n",
    "        'n_folds': N_FOLDS,\n",
    "        'random_seed': RANDOM_SEED\n",
    "    },\n",
    "    'activity_labels': ACTIVITY_LABELS,\n",
    "    'aggregate_results': aggregate_results,\n",
    "    'fold_results': all_fold_results,\n",
    "    'statistical_tests': statistical_tests,\n",
    "    'best_method': {\n",
    "        'name': best_method,\n",
    "        'display_name': method_names[best_method],\n",
    "        'accuracy_mean': aggregate_results[best_method]['accuracy_mean'],\n",
    "        'accuracy_std': aggregate_results[best_method]['accuracy_std'],\n",
    "        'f1_macro_mean': aggregate_results[best_method]['f1_macro_mean'],\n",
    "        'f1_macro_std': aggregate_results[best_method]['f1_macro_std']\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'final_results.json'), 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "# Create summary CSV for easy viewing\n",
    "summary_data = []\n",
    "for method in methods:\n",
    "    row = {\n",
    "        'Method': method_names[method],\n",
    "        'Accuracy_Mean': aggregate_results[method]['accuracy_mean'] * 100,\n",
    "        'Accuracy_Std': aggregate_results[method]['accuracy_std'] * 100,\n",
    "        'F1_Mean': aggregate_results[method]['f1_macro_mean'],\n",
    "        'F1_Std': aggregate_results[method]['f1_macro_std']\n",
    "    }\n",
    "    summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(os.path.join(OUTPUT_DIR, 'method_comparison.csv'), index=False)\n",
    "\n",
    "# Create robustness metrics CSV\n",
    "robustness_data = []\n",
    "for method in methods:\n",
    "    row = {\n",
    "        'Method': method_names[method],\n",
    "        'Worst_Task_Acc_Mean': aggregate_results[method]['worst_task_accuracy_mean'] * 100,\n",
    "        'Worst_Task_Acc_Std': aggregate_results[method]['worst_task_accuracy_std'] * 100,\n",
    "        'Cross_Task_Std_Mean': aggregate_results[method]['cross_task_std_mean'] * 100,\n",
    "        'Cross_Task_Std_Std': aggregate_results[method]['cross_task_std_std'] * 100\n",
    "    }\n",
    "    robustness_data.append(row)\n",
    "\n",
    "robustness_df = pd.DataFrame(robustness_data)\n",
    "robustness_df.to_csv(os.path.join(OUTPUT_DIR, 'robustness_metrics.csv'), index=False)\n",
    "\n",
    "# Create statistical tests CSV\n",
    "stats_data = []\n",
    "for comparison in ['fusion_vs_eeg', 'fusion_vs_har', 'fusion_vs_best_single']:\n",
    "    for metric in statistical_tests[comparison].keys():\n",
    "        row = {\n",
    "            'Comparison': comparison,\n",
    "            'Metric': metric,\n",
    "            'T_Statistic': statistical_tests[comparison][metric]['t_statistic'],\n",
    "            'P_Value': statistical_tests[comparison][metric]['p_value'],\n",
    "            'Significant': statistical_tests[comparison][metric]['p_value'] < 0.05\n",
    "        }\n",
    "        stats_data.append(row)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "stats_df.to_csv(os.path.join(OUTPUT_DIR, 'statistical_tests.csv'), index=False)\n",
    "\n",
    "# Create per-class summary CSV for best method\n",
    "per_class_data = []\n",
    "for class_name in ACTIVITY_LABELS:\n",
    "    metrics = aggregate_results[best_method]['per_class'][class_name]\n",
    "    row = {\n",
    "        'Class': class_name,\n",
    "        'Accuracy_Mean': metrics['accuracy_mean'] * 100,\n",
    "        'Accuracy_Std': metrics['accuracy_std'] * 100,\n",
    "        'Precision_Mean': metrics['precision_mean'],\n",
    "        'Precision_Std': metrics['precision_std'],\n",
    "        'Recall_Mean': metrics['recall_mean'],\n",
    "        'Recall_Std': metrics['recall_std'],\n",
    "        'F1_Mean': metrics['f1_mean'],\n",
    "        'F1_Std': metrics['f1_std'],\n",
    "        'AUC_Mean': metrics['auc_mean'],\n",
    "        'AUC_Std': metrics['auc_std']\n",
    "    }\n",
    "    per_class_data.append(row)\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class_data)\n",
    "per_class_df.to_csv(os.path.join(OUTPUT_DIR, f'{best_method}_per_class_results.csv'), index=False)\n",
    "\n",
    "# Export EEG-only per-class results\n",
    "eeg_per_class_data = []\n",
    "for class_name in ACTIVITY_LABELS:\n",
    "    metrics = aggregate_results['eeg']['per_class'][class_name]\n",
    "    row = {\n",
    "        'Class': class_name,\n",
    "        'Accuracy_Mean': metrics['accuracy_mean'] * 100,\n",
    "        'Accuracy_Std': metrics['accuracy_std'] * 100,\n",
    "        'Precision_Mean': metrics['precision_mean'],\n",
    "        'Precision_Std': metrics['precision_std'],\n",
    "        'Recall_Mean': metrics['recall_mean'],\n",
    "        'Recall_Std': metrics['recall_std'],\n",
    "        'F1_Mean': metrics['f1_mean'],\n",
    "        'F1_Std': metrics['f1_std'],\n",
    "        'AUC_Mean': metrics['auc_mean'],\n",
    "        'AUC_Std': metrics['auc_std']\n",
    "    }\n",
    "    eeg_per_class_data.append(row)\n",
    "\n",
    "eeg_per_class_df = pd.DataFrame(eeg_per_class_data)\n",
    "eeg_per_class_df.to_csv(os.path.join(OUTPUT_DIR, 'eeg_per_class_results.csv'), index=False)\n",
    "\n",
    "# Export HAR-only per-class results\n",
    "har_per_class_data = []\n",
    "for class_name in ACTIVITY_LABELS:\n",
    "    metrics = aggregate_results['har']['per_class'][class_name]\n",
    "    row = {\n",
    "        'Class': class_name,\n",
    "        'Accuracy_Mean': metrics['accuracy_mean'] * 100,\n",
    "        'Accuracy_Std': metrics['accuracy_std'] * 100,\n",
    "        'Precision_Mean': metrics['precision_mean'],\n",
    "        'Precision_Std': metrics['precision_std'],\n",
    "        'Recall_Mean': metrics['recall_mean'],\n",
    "        'Recall_Std': metrics['recall_std'],\n",
    "        'F1_Mean': metrics['f1_mean'],\n",
    "        'F1_Std': metrics['f1_std'],\n",
    "        'AUC_Mean': metrics['auc_mean'],\n",
    "        'AUC_Std': metrics['auc_std']\n",
    "    }\n",
    "    har_per_class_data.append(row)\n",
    "\n",
    "har_per_class_df = pd.DataFrame(har_per_class_data)\n",
    "har_per_class_df.to_csv(os.path.join(OUTPUT_DIR, 'har_per_class_results.csv'), index=False)\n",
    "\n",
    "print(f\"\\nResults saved to: {OUTPUT_DIR}\")\n",
    "print(f\"  - final_results.json: Complete results with all metrics\")\n",
    "print(f\"  - method_comparison.csv: Method comparison summary\")\n",
    "print(f\"  - robustness_metrics.csv: Worst-task accuracy and cross-task variance\")\n",
    "print(f\"  - statistical_tests.csv: Paired t-test results\")\n",
    "print(f\"  - eeg_per_class_results.csv: EEG-only per-class results\")  \n",
    "print(f\"  - har_per_class_results.csv: HAR-only per-class results\")  \n",
    "print(f\"  - {best_method}_per_class_results.csv: Per-class results for best method\")\n",
    "print(f\"  - fold_X/: Individual fold results and confusion matrices\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
